


import torch
import torchvision
from plotly.data import experiment
from sphinx.builders.gettext import timestamp
from torch import nn
from torchvision import transforms
from torchinfo import summary
import matplotlib.pyplot as plt
from tqdm import tqdm
from xlwings.utils import col_name

from going_modular import data_setup, engine
from going_modular.train import train_dataloader, test_dataloader, BATCH_SIZE

print(torch.__version__)
print(torchvision.__version__)


# Setup device agnostic code
device = torch.device("cuda" if torch.cuda.is_available() else "mps" if torch.mps.is_available() else "cpu")
device


# Set seeds
def set_seeds(seed: int=42):
    """
    Sets random sets for torch operations.

    Args:
        seed (int, optional): Random seed to set. Defaults to 42.
    """
    # the seed for general torch operations
    torch.manual_seed(seed)

    # Set the seed for CUDA+MPS torch operations (ones that happen on the GPU)
    torch.cuda.manual_seed(seed)
    torch.mps.manual_seed(seed)


set_seeds()





import os
import zipfile
from pathlib import Path
from typing import Optional

import requests
from tqdm.auto import tqdm


def download_data(
    source: str,
    destination: str,
    remove_source: bool = True,
    chunk_size: int = 1024
) -> Path:
    """Downloads a zipped dataset from source and unzips to destination."""

    data_path = Path("data")
    image_path = data_path / destination
    data_path.mkdir(parents=True, exist_ok=True)

    if image_path.is_dir():
        print(f"[INFO] {image_path} directory exists, skipping download.")
        return image_path

    print(f"[INFO] Creating directory {image_path}...")
    image_path.mkdir(parents=True, exist_ok=True)

    target_file = data_path / Path(source).name

    print(f"[INFO] Downloading {target_file.name}...")
    response = requests.get(source, stream=True)
    response.raise_for_status()

    total_size = int(response.headers.get("content-length", 0))

    with open(target_file, "wb") as f, tqdm(
        desc="Downloading",
        total=total_size,
        unit="B",
        unit_scale=True,
        unit_divisor=1024,
    ) as pbar:
        for chunk in response.iter_content(chunk_size=chunk_size):
            if chunk:
                f.write(chunk)
                pbar.update(len(chunk))


    print(f"[INFO] Unzipping {target_file.name}...")
    with zipfile.ZipFile(target_file, "r") as zip_ref:
        members = zip_ref.infolist()
        for member in tqdm(members, desc="Extracting", unit="file"):
            zip_ref.extract(member, image_path)

    if remove_source:
        target_file.unlink()

    print("[INFO] Download and extraction complete.")
    return image_path


image_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip",
                           destination="pizza_steak_sushi")
image_path








# Setup directories
train_dir = image_path / "train"
test_dir = image_path / "test"
train_dir, test_dir


# Setup ImageNet normalization levels
# See here: https://pytorch.org/vision/0.12/models.html
normalize = transforms.Normalize(mean=[0.485, 0.456, 0.4061], std=[0.229, 0.224, 0.225])

# Create transform pipeline manually
manual_transforms = transforms. Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    normalize
])
print (f"Manually created transforms: {manual_transforms}")

# Create DataLoaders
from going_modular import data_setup
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,
                                                                               test_dir=test_dir,
                                                                               train_transform=manual_transforms,
                                                                               test_transform=manual_transforms,
                                                                               batch_size=32)
train_dataloader, test_dataloader, class_names





# Setup dirs
train_dir = image_path / "train"
test_dir = image_path / "test"

# Setup pretrained weights
import torchvision
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # "DEFAULT" = best available

# Get the transforms from weights (these are the transforms used to train a particular or obtain a particular set of weights)
automatic_transforms = weights.transforms()
print (f"Automatically created transforms: {automatic_transforms}")
# Create DataLoaders
train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,
                                                                               test_dir=test_dir,
                                                                               train_transform=automatic_transforms,
                                                                               test_transform=automatic_transforms,
                                                                               batch_size=32)
train_dataloader, test_dataloader, class_names





weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # "DEFAULT" = best available
model = torchvision.models.efficientnet_b0(weights= weights).to(device)


# Freeze all base layers by setting their requires_grad attribute to False
for param in model.features.parameters():
    # print(param)
    param.requires_grad = False


# Adjust the classifier head
set_seeds()
model.classifier = nn.Sequential(
    nn.Dropout(p=0.2, inplace=True),
    nn.Linear(in_features=1280, out_features=len(class_names), bias=True)
).to(device)


from torchinfo import summary

summary(model,
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])





# Define loss function optimizer
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)





# Setup a SummaryWriter
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
writer


from tqdm.auto import tqdm
from typing import Dict, List, Tuple

from going_modular.engine import train_step, test_step

def train(model: torch.nn.Module,
          train_dataloader: torch.utils.data.DataLoader,
          test_dataloader: torch.utils.data.DataLoader,
          optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module,
          epochs: int,
          device: torch.device,
          writer: SummaryWriter = writer) -> Dict[str, List]:
    """Trains and tests a PyTorch model.

    Passes a target PyTorch models through train_step() and test_step()
    functions for a number of epochs, training and testing the model
    in the same epoch loop.

    Calculates, prints and stores evaluation metrics throughout.

    Args:
        model: A PyTorch model to be trained and tested.
        train_dataloader: A DataLoader instance for the model to be trained on.
        test_dataloader: A DataLoader instance for the model to be tested on.
        optimizer: A PyTorch optimizer to help minimize the loss function.
        loss_fn: A PyTorch loss function to calculate loss on both datasets.
        epochs: An integer indicating how many epochs to train for.
        device: A target device to compute on (e.g. "cuda" or "cpu").

    Returns:
        A dictionary of training and testing loss as well as training and
        testing accuracy metrics. Each metric has a value in a list for
        each epoch.
        In the form: {train_loss: [...],
        train_acc: [...],
        test_loss: [...],
        test_acc: [...]}

    For example if training for epochs=2:
        {train_loss: [2.0616, 1.0537],
        train_acc: [0.3945, 0.3945],
        test_loss: [1.2641, 1.5706],
        test_acc: [0.3400, 0.2973]}
    """
    # Create empty results dictionary
    results = {"train_loss": [],
               "train_acc": [],
               "test_loss": [],
               "test_acc": []
               }

    model.to(device)
    # Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = train_step(model=model,
                                           dataloader=train_dataloader,
                                           loss_fn=loss_fn,
                                           optimizer=optimizer,
                                           device=device)
        test_loss, test_acc = test_step(model=model,
                                        dataloader=test_dataloader,
                                        loss_fn=loss_fn,
                                        device=device)

        # Print out what's happening
        print(f"Epoch: {epoch+1} | "
              f"train_loss: {train_loss:.4f} | "
              f"train_acc: {train_acc:.4f} | "
              f"test_loss: {test_loss:.4f} | "
              f"test_acc: {test_acc:.4f}"
              )

        # Update results dictionary
        results["train_loss"].append(train_loss)
        results["train_acc"].append(train_acc)
        results["test_loss"].append(test_loss)
        results["test_acc"].append(test_acc)


        ### New: Experiment tracking
        writer.add_scalars(main_tag="Loss",
                          tag_scalar_dict={"train_loss":train_loss,
                                           "test_loss": test_loss},
                          global_step=epoch)
        writer.add_scalars(main_tag="Accuracy",
                          tag_scalar_dict={"train_acc":train_acc,
                                           "test_acc": test_acc},
                          global_step=epoch)
    writer.add_graph(model=model,
                     input_to_model=torch.randn(32, 3, 224, 224).to(device))

    # Close the writer
    writer.close()
    ## End new ##

    # Return the filled results at the end of the epochs
    return results


# Train model
set_seeds()
results = train(model=model,
                train_dataloader=train_dataloader,
                test_dataloader=test_dataloader,
                optimizer=optimizer,
                loss_fn=loss_fn,
                epochs=5,
                device=device)


results





import shutil
import sys
import os

print("Python:", sys.executable)
print("TensorBoard:", shutil.which("tensorboard"))



import os
import sys

tb_path = os.path.join(os.path.dirname(sys.executable), "tensorboard")
os.environ["TENSORBOARD_BINARY"] = tb_path

print("Using TensorBoard at:", tb_path)



%reload_ext tensorboard


%tensorboard --logdir runs





from torch.utils.tensorboard import SummaryWriter
def create_writer(experiment_name: str,
                  model_name: str,
                  extra: str = None) -> SummaryWriter:
    """
    Creates a torch.utils.tensorboard.SummaryWriter instance tracking to a specific directory.

    Args:
        experiment_name: The name of the experiment.
        model_name: The name of the model.
        extra: An optional string to append to the experiment name.

    Returns:
        A SummaryWriter instance.
    """
    from datetime import datetime
    import os

    # Get timestamp of current date in reverse order
    timestamp = datetime.now().strftime("%Y-%m-%d")

    if extra:
        # Create log directory path
        log_dir = os.path.join("runs", timestamp, experiment_name, model_name, extra)
    else:
        log_dir = os.path.join("runs", timestamp, experiment_name, model_name)

    print(f"[INFO] Created SummaryWriter saving to {log_dir}")
    return SummaryWriter(log_dir=log_dir)


from datetime import datetime
timestamp = datetime.now().strftime("%Y-%m-%d")
timestamp


example_writer = create_writer(experiment_name="data_10_percent",
                               model_name="effnetb0",
                               extra="5_epochs")
example_writer





from tqdm.auto import tqdm
from typing import Dict, List, Tuple

from going_modular.engine import train_step, test_step

def train(model: torch.nn.Module,
          train_dataloader: torch.utils.data.DataLoader,
          test_dataloader: torch.utils.data.DataLoader,
          optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module,
          epochs: int,
          device: torch.device,
          writer: SummaryWriter = None) -> Dict[str, List]:
    """Trains and tests a PyTorch model.

    Passes a target PyTorch models through train_step() and test_step()
    functions for a number of epochs, training and testing the model
    in the same epoch loop.

    Calculates, prints and stores evaluation metrics throughout.

    Args:
        model: A PyTorch model to be trained and tested.
        train_dataloader: A DataLoader instance for the model to be trained on.
        test_dataloader: A DataLoader instance for the model to be tested on.
        optimizer: A PyTorch optimizer to help minimize the loss function.
        loss_fn: A PyTorch loss function to calculate loss on both datasets.
        epochs: An integer indicating how many epochs to train for.
        device: A target device to compute on (e.g. "cuda" or "cpu").

    Returns:
        A dictionary of training and testing loss as well as training and
        testing accuracy metrics. Each metric has a value in a list for
        each epoch.
        In the form: {train_loss: [...],
        train_acc: [...],
        test_loss: [...],
        test_acc: [...]}

    For example if training for epochs=2:
        {train_loss: [2.0616, 1.0537],
        train_acc: [0.3945, 0.3945],
        test_loss: [1.2641, 1.5706],
        test_acc: [0.3400, 0.2973]}
    """
    # Create empty results dictionary
    results = {"train_loss": [],
               "train_acc": [],
               "test_loss": [],
               "test_acc": []
               }

    model.to(device)
    # Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = train_step(model=model,
                                           dataloader=train_dataloader,
                                           loss_fn=loss_fn,
                                           optimizer=optimizer,
                                           device=device)
        test_loss, test_acc = test_step(model=model,
                                        dataloader=test_dataloader,
                                        loss_fn=loss_fn,
                                        device=device)

        # Print out what's happening
        print(f"Epoch: {epoch+1} | "
              f"train_loss: {train_loss:.4f} | "
              f"train_acc: {train_acc:.4f} | "
              f"test_loss: {test_loss:.4f} | "
              f"test_acc: {test_acc:.4f}"
              )

        # Update results dictionary
        results["train_loss"].append(train_loss)
        results["train_acc"].append(train_acc)
        results["test_loss"].append(test_loss)
        results["test_acc"].append(test_acc)


        ### New: Experiment tracking
        if writer:
            writer.add_scalars(main_tag="Loss",
                               tag_scalar_dict={"train_loss":train_loss,
                                               "test_loss": test_loss},
                               global_step=epoch)
            writer.add_scalars(main_tag="Accuracy",
                               tag_scalar_dict={"train_acc":train_acc,
                                               "test_acc": test_acc},
                               global_step=epoch)
            writer.add_graph(model=model,
                             input_to_model=torch.randn(32, 3, 224, 224).to(device))

            # Close the writer
            writer.close()
    # Return the filled results at the end of the epochs
    return results














# Download 10% and 20% datasets
data_10_percent_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip",
                                     destination="pizza_steak_sushi")

data_20_percent_path = download_data(source="https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip",
                                     destination="pizza_steak_sushi_20_percent")





# Setup training directory paths
train_dir_10_percent = data_10_percent_path / "train"
train_dir_20_percent = data_20_percent_path / "train"

# Setup the test directory
test_dir = data_10_percent_path / "test"

train_dir_10_percent, train_dir_20_percent, test_dir


weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # "DEFAULT" = best available

# Get the transforms from weights (these are the transforms used to train a particular or obtain a particular set of weights)
simple_transform = weights.transforms()
simple_transform


BATCH_SIZE = 32

# Create 10% training and test Dataloaders
train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,
                                                                               test_dir=test_dir,
                                                                               train_transform=simple_transform,
                                                                               test_transform=simple_transform,
                                                                               batch_size=BATCH_SIZE)
train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,
                                                                               test_dir=test_dir,
                                                                               train_transform=simple_transform,
                                                                               test_transform=simple_transform,
                                                                               batch_size=BATCH_SIZE)

print(f"Number of batches: {len(train_dataloader_10_percent)=}")
print(f"Number of batches: {len(train_dataloader_20_percent)=}")
print(f"Number of batches: {len(test_dataloader)=}")
print(f"Class names: {class_names}")





import torchvision

# Create an EffNetB2
effnetB2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # "DEFAULT" = best available
effnetB2 = torchvision.models.efficientnet_b2(weights=effnetB2_weights).to(device)

effnetB2


summary(effnetB2,
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])


import torchvision
from torch import nn

OUT_FEATURES = len(class_names)

# Create an EffNetB0
def create_effnetB0(device=device):
    # Get the weights and setup a model
    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # "DEFAULT" = best available
    model = torchvision.models.efficientnet_b0(weights=weights).to(device)

    # Freeze all base model layers
    for param in model.features.parameters():
        param.requires_grad = False
      # Change the classifier head

    set_seeds()
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.2, inplace=True),
        nn.Linear(in_features=1280, out_features=len(class_names), bias=True)
    ).to(device)

    # Give the model a name
    model.name = "effnetB0"
    print(f"[INFO] Created new model: {model.name}")
    return model

# Create an EffNetB2
def create_effnetB2(device=device):
    # Get the weights and setup a model
    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # "DEFAULT" = best available
    model = torchvision.models.efficientnet_b2(weights=weights).to(device)

    # Freeze all base model layers
    for param in model.features.parameters():
        param.requires_grad = False
      # Change the classifier head

    set_seeds()
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.3, inplace=True),
        nn.Linear(in_features=1408, out_features=len(class_names), bias=True)
    ).to(device)

    # Give the model a name
    model.name = "effnetB2"
    print(f"[INFO] Created new model: {model.name}")
    return model


effnetB2.classifier


created_model_test_effnetB0 = create_effnetB0()
created_model_test_effnetB2 = create_effnetB2()


summary(created_model_test_effnetB0,
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])


summary(created_model_test_effnetB2,
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])





# Create epoch list
num_epochs = [5, 10]

# Create models list ( need to create a new model for each experiment)
models = ["effnetB0", "effnetB2"]

# Create a DataLoaders dictionary
train_dataloaders = {"data_10_percent": train_dataloader_10_percent,
                     "data_20_percent": train_dataloader_20_percent}


%%time
from going_modular.utils import save_model

# Set seeds
set_seeds()

# Keep track of experiment numbers
experiment_number = 0

# Loop through each DataLoader
for dataloader_name, train_dataloader in train_dataloaders.items():
    # Loop through the epochs
    for epochs in num_epochs:
        # Loop through each model name and create a new model instance
        for model_name in models:
            if experiment_number in range(6):
                experiment_number+=1
                continue
            # Print out the info
            print(f"[INFO] Experiment {experiment_number} | Number of Epochs {epochs} | Model: {model_name} | DataLoader: {dataloader_name}")

            if model_name == "effnetB0":
                model = create_effnetB0(device=device)
            else:
                model = create_effnetB2(device=device)

            # Create a new loss and optimizer
            loss_fn = nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(lr=0.001, params=model.parameters())

            # Train target model with target dataloader and track experiments
            writer = create_writer(
                experiment_name= dataloader_name,
                model_name=model_name,
                extra=f"{epochs}_epochs",
            )

            train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer)

            # Save the model to file so we can import it late if needed
            save_filepath = f"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth"
            save_model(model=model,
                       target_dir="models",
                       model_name=save_filepath)
            del model
            print("-"*50 + "\n")
            experiment_number += 1

















# Let's view our experiments within TensorBoard from within the notebook
%load_ext tensorboard
%tensorboard --logdir runs





# Upload the results to TensorBoard.dev (uncomment to try it out)
!tensorboard dev upload --logdir runs --name "07. PyTorch Experiment Tracking: FoodVision Mini model result" --description "Comparing results of different model size, training data amount and training time."








# Setup best model file path
best_model_path = ""

# Instantiate a new instance of EFFnetB? (to load the saved state_dict())
best_model = create_effnetB0()

# Load the saved best model state_dict()
best_model.load_state_dict(torch.load(best_model_path))





# Check the model file size
from pathlib import Path

# Get the model sioze in bytes then convert it to megabytes
effnetb2_model_size = Path(best_model_path).stat().st_size // (1_048_576)
print(f"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB")


# Import function to make prediction on images and plot them
from going_modular.predictions import pred_and_plot_image

# Get a random list of 3 image path names from the test dataset
import random
num_images_to_plot = 3
test_image_paths_list = list(Path(data_20_percent_path/ "test").glob("*/*.jpg"))
test_image_paths_sample = random.sample(test_image_paths_list,
                                       k=num_images_to_plot)

for image_path in test_image_paths_sample:
    pred_and_plot_image(
        model=best_model,
        image_path=image_path,
        image_size=(224,224),
        class_names=class_names,
    )





pred_and_plot_image(
    model=best_model,
    image_path=Path("data/04-pizza-dad.jpeg"),
    class_names=class_names,
)





from Progress.course_progress_func import update_progress, progress_pie_chart, monthly_progress, progress_report_print
update_progress(video_index=221 , done=True)


progress_report_print()
