{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bc2752f96edabe",
   "metadata": {},
   "source": [
    "# 07. PyTorch Experiment Tracking\n",
    "\n",
    "Machine learning is very experimental.\n",
    "\n",
    "In order to figure out which experiments are worth pursuing, that's where **experiment tracking** comes in, it helps you to figure out what doesn't work so you can figure out what **does** work.\n",
    "\n",
    "In this notebook, we're going to see an example of programmatically tracking experiments.\n",
    "\n",
    "Resources:\n",
    "* Book version of notebook: https://www.learnpytorch.io/07_pytorch_experiment_tracking/\n",
    "* Ask a question: https://github.com/mrdbourke/pytorch-deep-learning/discussions\n",
    "* Extra-curriculum: https://madewithml.com/courses/mlops/experiment-tracking/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c296b9c0199bb73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:02:36.690857Z",
     "start_time": "2025-12-15T09:02:20.483538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n",
      "0.20.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from plotly.data import experiment\n",
    "from sphinx.builders.gettext import timestamp\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from xlwings.utils import col_name\n",
    "\n",
    "from going_modular import data_setup, engine\n",
    "from going_modular.train import train_dataloader, test_dataloader, BATCH_SIZE\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff5f67095c30f1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:02:36.755585Z",
     "start_time": "2025-12-15T09:02:36.696982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13759b13f5afebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:02:36.785553Z",
     "start_time": "2025-12-15T09:02:36.772879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "def set_seeds(seed: int=42):\n",
    "    \"\"\"\n",
    "    Sets random sets for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed to set. Defaults to 42.\n",
    "    \"\"\"\n",
    "    # the seed for general torch operations\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set the seed for CUDA+MPS torch operations (ones that happen on the GPU)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.mps.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6431fcd37a019d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:02:36.806705Z",
     "start_time": "2025-12-15T09:02:36.786902Z"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcae76b88b261d9",
   "metadata": {},
   "source": [
    "## 1. Get data\n",
    "\n",
    "Want to get pizza, steak, sushi images.\n",
    "\n",
    "So we can run experiments building FoodVision Mini and see which model performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc31f177819d785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:43.812953Z",
     "start_time": "2025-12-15T09:03:43.473520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/pizza_steak_sushi')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def download_data(\n",
    "    source: str,\n",
    "    destination: str,\n",
    "    remove_source: bool = True,\n",
    "    chunk_size: int = 1024\n",
    ") -> Path:\n",
    "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\"\"\"\n",
    "\n",
    "    data_path = Path(\"data\")\n",
    "    image_path = data_path / destination\n",
    "    data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if image_path.is_dir():\n",
    "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
    "        return image_path\n",
    "\n",
    "    print(f\"[INFO] Creating directory {image_path}...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    target_file = data_path / Path(source).name\n",
    "\n",
    "    print(f\"[INFO] Downloading {target_file.name}...\")\n",
    "    response = requests.get(source, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    with open(target_file, \"wb\") as f, tqdm(\n",
    "        desc=\"Downloading\",\n",
    "        total=total_size,\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "    print(f\"[INFO] Unzipping {target_file.name}...\")\n",
    "    with zipfile.ZipFile(target_file, \"r\") as zip_ref:\n",
    "        members = zip_ref.infolist()\n",
    "        for member in tqdm(members, desc=\"Extracting\", unit=\"file\"):\n",
    "            zip_ref.extract(member, image_path)\n",
    "\n",
    "    if remove_source:\n",
    "        target_file.unlink()\n",
    "\n",
    "    print(\"[INFO] Download and extraction complete.\")\n",
    "    return image_path\n",
    "\n",
    "\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32614c0061f07853",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T12:57:12.731851Z",
     "start_time": "2025-12-13T12:57:12.678823Z"
    }
   },
   "source": [
    "## 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea661e4956f9cdd",
   "metadata": {},
   "source": [
    "### 2.1 Create DataLoaders with manual transforms\n",
    "The goal with transforms is to ensure your custom data is formatted in a reproducible way as well as a way that will suit pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a771ab332a26c76d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.164835Z",
     "start_time": "2025-12-14T18:07:52.115952Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup directories\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de637faa45e05155",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.199631Z",
     "start_time": "2025-12-14T18:07:52.166448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.4061], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x30514ac30>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x30461d2b0>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup ImageNet normalization levels\n",
    "# See here: https://pytorch.org/vision/0.12/models.html\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.4061], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Create transform pipeline manually\n",
    "manual_transforms = transforms. Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "print (f\"Manually created transforms: {manual_transforms}\")\n",
    "\n",
    "# Create DataLoaders\n",
    "from going_modular import data_setup\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               train_transform=manual_transforms,\n",
    "                                                                               test_transform=manual_transforms,\n",
    "                                                                               batch_size=32)\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa459529f16c7a0",
   "metadata": {},
   "source": [
    "### 2.2 Create DataLoaders using automatically created transforms\n",
    "\n",
    "The same principle applies for automatically created transforms we want our custom data in the same format as the pretrained data the model trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c08dabc7cdb0013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.232264Z",
     "start_time": "2025-12-14T18:07:52.204450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created transforms: ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x3051d20f0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x3051d21b0>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup dirs\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "# Setup pretrained weights\n",
    "import torchvision\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # \"DEFAULT\" = best available\n",
    "\n",
    "# Get the transforms from weights (these are the transforms used to train a particular or obtain a particular set of weights)\n",
    "automatic_transforms = weights.transforms()\n",
    "print (f\"Automatically created transforms: {automatic_transforms}\")\n",
    "# Create DataLoaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               train_transform=automatic_transforms,\n",
    "                                                                               test_transform=automatic_transforms,\n",
    "                                                                               batch_size=32)\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84617dd806234ec",
   "metadata": {},
   "source": [
    "## 3. Getting a pretrained model, freeze the base layers and change the classifier head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5051895887231542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.480155Z",
     "start_time": "2025-12-14T18:07:52.232932Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # \"DEFAULT\" = best available\n",
    "model = torchvision.models.efficientnet_b0(weights= weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b79496fc5f7ad63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.497385Z",
     "start_time": "2025-12-14T18:07:52.481169Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze all base layers by setting their requires_grad attribute to False\n",
    "for param in model.features.parameters():\n",
    "    # print(param)\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b02df0257f6d268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:52.506955Z",
     "start_time": "2025-12-14T18:07:52.498456Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adjust the classifier head\n",
    "set_seeds()\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a98cb5303a9b784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:55.454003Z",
     "start_time": "2025-12-14T18:07:52.507574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59159ebcbd027628",
   "metadata": {},
   "source": [
    "## 4. train a single model and track results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab632516600912e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:55.512777Z",
     "start_time": "2025-12-14T18:07:55.493795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss function optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2fc23f8eecc39",
   "metadata": {},
   "source": [
    "To track experiments, we're going to use TensorBoard: https://www.tensorflow.org/tensorboard\n",
    "\n",
    "And to interact with IensorBoard, we can use PyTorch's Summarywriter - https:// pytorch.org/docs/stable/tensorboard.html\n",
    "* Also see here: https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.Summarywriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac60dd2a21ed3d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:07:55.911215Z",
     "start_time": "2025-12-14T18:07:55.514571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x305e2b290>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup a SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c264137383625677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:02:36.919431Z",
     "start_time": "2025-12-15T09:02:36.808247Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoing_modular\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_step, test_step\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m      7\u001b[0m           train_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[1;32m      8\u001b[0m           test_dataloader: torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader,\n\u001b[1;32m      9\u001b[0m           optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,loss_fn: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     10\u001b[0m           epochs: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     11\u001b[0m           device: torch\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[0;32m---> 12\u001b[0m           writer: SummaryWriter \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, List]:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Trains and tests a PyTorch model.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Passes a target PyTorch models through train_step() and test_step()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m        test_acc: [0.3400, 0.2973]}\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Create empty results dictionary\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'writer' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          writer: SummaryWriter = writer) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained and tested.\n",
    "        train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "        test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "        loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "        epochs: An integer indicating how many epochs to train for.\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics. Each metric has a value in a list for\n",
    "        each epoch.\n",
    "        In the form: {train_loss: [...],\n",
    "        train_acc: [...],\n",
    "        test_loss: [...],\n",
    "        test_acc: [...]}\n",
    "\n",
    "    For example if training for epochs=2:\n",
    "        {train_loss: [2.0616, 1.0537],\n",
    "        train_acc: [0.3945, 0.3945],\n",
    "        test_loss: [1.2641, 1.5706],\n",
    "        test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "               }\n",
    "\n",
    "    model.to(device)\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"train_loss: {train_loss:.4f} | \"\n",
    "              f\"train_acc: {train_acc:.4f} | \"\n",
    "              f\"test_loss: {test_loss:.4f} | \"\n",
    "              f\"test_acc: {test_acc:.4f}\"\n",
    "              )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "        ### New: Experiment tracking\n",
    "        writer.add_scalars(main_tag=\"Loss\",\n",
    "                          tag_scalar_dict={\"train_loss\":train_loss,\n",
    "                                           \"test_loss\": test_loss},\n",
    "                          global_step=epoch)\n",
    "        writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                          tag_scalar_dict={\"train_acc\":train_acc,\n",
    "                                           \"test_acc\": test_acc},\n",
    "                          global_step=epoch)\n",
    "    writer.add_graph(model=model,\n",
    "                     input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "\n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    ## End new ##\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec794d204067b428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:14:23.701589Z",
     "start_time": "2025-12-14T18:07:55.927998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e63c1ed89a486cbd05a9870d287575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0823 | train_acc: 0.4062 | test_loss: 0.8991 | test_acc: 0.5909\n",
      "Epoch: 2 | train_loss: 0.8564 | train_acc: 0.7695 | test_loss: 0.7927 | test_acc: 0.8456\n",
      "Epoch: 3 | train_loss: 0.7914 | train_acc: 0.7891 | test_loss: 0.7373 | test_acc: 0.8561\n",
      "Epoch: 4 | train_loss: 0.7206 | train_acc: 0.7500 | test_loss: 0.6338 | test_acc: 0.8759\n",
      "Epoch: 5 | train_loss: 0.6368 | train_acc: 0.7812 | test_loss: 0.6190 | test_acc: 0.8665\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "set_seeds()\n",
    "results = train(model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=5,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3be4db1930488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:14:23.991764Z",
     "start_time": "2025-12-14T18:14:23.961800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [1.0823059901595116,\n",
       "  0.8563981279730797,\n",
       "  0.791380912065506,\n",
       "  0.7206062972545624,\n",
       "  0.636790856719017],\n",
       " 'train_acc': [0.40625, 0.76953125, 0.7890625, 0.75, 0.78125],\n",
       " 'test_loss': [0.8990757266680399,\n",
       "  0.792700986067454,\n",
       "  0.7373119592666626,\n",
       "  0.6338079373041788,\n",
       "  0.6189916928609213],\n",
       " 'test_acc': [0.5909090909090909,\n",
       "  0.8456439393939394,\n",
       "  0.8560606060606061,\n",
       "  0.8759469696969697,\n",
       "  0.8664772727272728]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b23e9006f25c13",
   "metadata": {},
   "source": [
    "## 5. View our models results with TensorBoard\n",
    "\n",
    "There are a few ways to view TensorBoard results:\n",
    "https://www.tensorflow.org/tensorboard/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9878f9f5a3de030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"TensorBoard:\", shutil.which(\"tensorboard\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2680a56952530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "tb_path = os.path.join(os.path.dirname(sys.executable), \"tensorboard\")\n",
    "os.environ[\"TENSORBOARD_BINARY\"] = tb_path\n",
    "\n",
    "print(\"Using TensorBoard at:\", tb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca820dd12bebc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "882a8d4149a769a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:30:16.335214Z",
     "start_time": "2025-12-14T18:30:13.359621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-abd2141a564a3bef\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-abd2141a564a3bef\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "jetTransient": {
      "display_id": "294cf953ca73244de26002d17fffbf4a"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186433f51c371067",
   "metadata": {},
   "source": [
    "## 6. Create a function to prepare `SummaryWriter()` instance\n",
    "\n",
    "By default, our `SummaryWriter()` class saves to `runs`.\n",
    "\n",
    "How about if we wanted to save different examples to different folders?\n",
    "\n",
    "In essence, one experiment = one folder.\n",
    "\n",
    "For example, we'd like to track:\n",
    "* Experiment date/timestamp\n",
    "* Experiment name\n",
    "* Model name\n",
    "* Extra - is there anything else that should be tracked?\n",
    "\n",
    "Let's create a function to create a `SummaryWriter()` instance to take all of these things into account.\n",
    "\n",
    "So ideally we end up tracking experiments to a directory:\n",
    "\n",
    "`runs/YYYY-MM-DD/experiment_name/model_name/extra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf60f1c88468cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:26.761813Z",
     "start_time": "2025-12-15T09:03:25.640168Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "def create_writer(experiment_name: str,\n",
    "                  model_name: str,\n",
    "                  extra: str = None) -> SummaryWriter:\n",
    "    \"\"\"\n",
    "    Creates a torch.utils.tensorboard.SummaryWriter instance tracking to a specific directory.\n",
    "\n",
    "    Args:\n",
    "        experiment_name: The name of the experiment.\n",
    "        model_name: The name of the model.\n",
    "        extra: An optional string to append to the experiment name.\n",
    "\n",
    "    Returns:\n",
    "        A SummaryWriter instance.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # Get timestamp of current date in reverse order\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "\n",
    "    print(f\"[INFO] Created SummaryWriter saving to {log_dir}\")\n",
    "    return SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ad4cb5e1f8ac824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:14:28.574004Z",
     "start_time": "2025-12-14T18:14:28.565802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-12-14'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9e29f95e436fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:14:28.556490Z",
     "start_time": "2025-12-14T18:14:28.535670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created SummaryWriter saving to runs/2025-12-14/data_10_percent/effnetb0/5_epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x318da6840>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb0\",\n",
    "                               extra=\"5_epochs\")\n",
    "example_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bade640d99b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:14:28.565340Z",
     "start_time": "2025-12-14T18:14:28.557037Z"
    }
   },
   "source": [
    "### 6.1 Update the `train()` function to include a writer parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acb6ca24c416dda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:34.168742Z",
     "start_time": "2025-12-15T09:03:34.119824Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from going_modular.engine import train_step, test_step\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          writer: SummaryWriter = None) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained and tested.\n",
    "        train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "        test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "        loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "        epochs: An integer indicating how many epochs to train for.\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of training and testing loss as well as training and\n",
    "        testing accuracy metrics. Each metric has a value in a list for\n",
    "        each epoch.\n",
    "        In the form: {train_loss: [...],\n",
    "        train_acc: [...],\n",
    "        test_loss: [...],\n",
    "        test_acc: [...]}\n",
    "\n",
    "    For example if training for epochs=2:\n",
    "        {train_loss: [2.0616, 1.0537],\n",
    "        train_acc: [0.3945, 0.3945],\n",
    "        test_loss: [1.2641, 1.5706],\n",
    "        test_acc: [0.3400, 0.2973]}\n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "               }\n",
    "\n",
    "    model.to(device)\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(f\"Epoch: {epoch+1} | \"\n",
    "              f\"train_loss: {train_loss:.4f} | \"\n",
    "              f\"train_acc: {train_acc:.4f} | \"\n",
    "              f\"test_loss: {test_loss:.4f} | \"\n",
    "              f\"test_acc: {test_acc:.4f}\"\n",
    "              )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "        ### New: Experiment tracking\n",
    "        if writer:\n",
    "            writer.add_scalars(main_tag=\"Loss\",\n",
    "                               tag_scalar_dict={\"train_loss\":train_loss,\n",
    "                                               \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\",\n",
    "                               tag_scalar_dict={\"train_acc\":train_acc,\n",
    "                                               \"test_acc\": test_acc},\n",
    "                               global_step=epoch)\n",
    "            writer.add_graph(model=model,\n",
    "                             input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486fccfa39c8e02",
   "metadata": {},
   "source": [
    "## 7. Setting up a series of modelling experiments\n",
    "* Challenge: Setup 2x modeling experiments with efnetto, pizza, steak sushi data and train one model for 5 epochs and another model for 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea185a86777d9bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:18:25.021980Z",
     "start_time": "2025-12-14T18:18:24.796073Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 7.1 What kind of experiments should you run?\n",
    "\n",
    "The number of machine learning experiments you can run, is like the number of different models you can build... almost limitless.\n",
    "\n",
    "However, you can't test everything...\n",
    "\n",
    "So what should you test?\n",
    "* Change the number of epochs\n",
    "* Change the number of hidden layers/units\n",
    "* Change the amount of data (right now we're using 10% of the Food101 dataset for pizza, steak, sushi)\n",
    "* Change the learning rate\n",
    "* Try different kinds of data augmentation\n",
    "* Choose a different model architecture\n",
    "\n",
    "This is why transfer learning is powerful, because it's a working model that you can apply to your own problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898758441c50957",
   "metadata": {},
   "source": [
    "### 7.2 What experiments are we going to run?\n",
    "\n",
    "We're going to turn 3 dials:\n",
    "1. Model size - EffnetB0 vs EffnetB2 (in terms of number of params)\n",
    "2. Dataset size - 10% of pizza, steak, sushi images vs 20% (generally more data = better results)\n",
    "3. Training time - 5 epochs vs 10 epochs (generally longer training time = better\n",
    "results... before the model starts to overfit)\n",
    "\n",
    "To begin, we're still keeping things relatively small so that our experiments run quickly.\n",
    "\n",
    "**Our goal:** a model that is well performing but still small enough to run on a mobile device or web browser, so FoodVision Mini can come to life.\n",
    "\n",
    "If you had infinite compute + time, you should basically always choose the biggest model and biggest dataset you can.\n",
    "http://www.incompleteideas.net/Incldeas/BitterLesson.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698abb1ac6639d1f",
   "metadata": {},
   "source": [
    "### 7.3 Download different dataset\n",
    "\n",
    "We want two datasets:\n",
    "\n",
    "1. Pizza, steak, sushi 10% - https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n",
    "2. Pizza, steak, sushi 20% - https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\n",
    "\n",
    "They were created with: https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "914796255618e1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:56.600420Z",
     "start_time": "2025-12-15T09:03:56.528954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download 10% and 20% datasets\n",
    "data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                                     destination=\"pizza_steak_sushi\")\n",
    "\n",
    "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                                     destination=\"pizza_steak_sushi_20_percent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac05adfb1b218de",
   "metadata": {},
   "source": [
    "### 7.4 Transform Datasets and Create DataLoaders\n",
    "\n",
    "We'll need to transform our data in a few ways:\n",
    "1. Resize the images to (224, 224)\n",
    "2. Make sure image tensor values are between [0, 1]\n",
    "3. Normalize the images so they have the same data distribution as ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1d09a8fb1f1cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:57.393854Z",
     "start_time": "2025-12-15T09:03:57.353973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/pizza_steak_sushi/train'),\n",
       " PosixPath('data/pizza_steak_sushi_20_percent/train'),\n",
       " PosixPath('data/pizza_steak_sushi/test'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup training directory paths\n",
    "train_dir_10_percent = data_10_percent_path / \"train\"\n",
    "train_dir_20_percent = data_20_percent_path / \"train\"\n",
    "\n",
    "# Setup the test directory\n",
    "test_dir = data_10_percent_path / \"test\"\n",
    "\n",
    "train_dir_10_percent, train_dir_20_percent, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51209fedb6df1e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:58.232746Z",
     "start_time": "2025-12-15T09:03:58.134534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[288]\n",
       "    resize_size=[288]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" = best available\n",
    "\n",
    "# Get the transforms from weights (these are the transforms used to train a particular or obtain a particular set of weights)\n",
    "simple_transform = weights.transforms()\n",
    "simple_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd4522261d0207af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:03:58.677200Z",
     "start_time": "2025-12-15T09:03:58.626813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: len(train_dataloader_10_percent)=8\n",
      "Number of batches: len(train_dataloader_20_percent)=15\n",
      "Number of batches: len(test_dataloader)=3\n",
      "Class names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create 10% training and test Dataloaders\n",
    "train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               train_transform=simple_transform,\n",
    "                                                                               test_transform=simple_transform,\n",
    "                                                                               batch_size=BATCH_SIZE)\n",
    "train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               train_transform=simple_transform,\n",
    "                                                                               test_transform=simple_transform,\n",
    "                                                                               batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Number of batches: {len(train_dataloader_10_percent)=}\")\n",
    "print(f\"Number of batches: {len(train_dataloader_20_percent)=}\")\n",
    "print(f\"Number of batches: {len(test_dataloader)=}\")\n",
    "print(f\"Class names: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c742bf9f8d182444",
   "metadata": {},
   "source": [
    "### 7.5 Create feature extractor models\n",
    "We want two functions:\n",
    "1. Creates a `torchvision.models efficientnet_b0()` feature extractor with a frozen backbone/base layers and a custom classifier head (EffNetB0).\n",
    "2. Creates a `torchvision.models.efficientnet_b2()` feature extractor with a frozen backbone/base layers and a custom classifier head (EffNetB2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e02be8fce104a0a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:04:00.294372Z",
     "start_time": "2025-12-15T09:03:59.858641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n",
       "            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n",
       "            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n",
       "            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n",
       "            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=True)\n",
       "    (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# Create an EffNetB2\n",
    "effnetB2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # \"DEFAULT\" = best available\n",
    "effnetB2 = torchvision.models.efficientnet_b2(weights=effnetB2_weights).to(device)\n",
    "\n",
    "effnetB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1899114a88962eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:04:04.778953Z",
     "start_time": "2025-12-15T09:04:00.816223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1408, 7, 7]     --                   True\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112]   [32, 16, 112, 112]   612                  True\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 48, 28, 28]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 48, 28, 28]     16,518               True\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     43,308               True\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     43,308               True\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28]     [32, 88, 14, 14]     --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28]     [32, 88, 14, 14]     50,300               True\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     123,750              True\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14]     [32, 120, 14, 14]    --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14]     [32, 120, 14, 14]    149,158              True\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    237,870              True\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14]    [32, 208, 7, 7]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14]    [32, 208, 7, 7]      301,406              True\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      686,868              True\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7]      [32, 352, 7, 7]      --                   True\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7]      [32, 352, 7, 7]      846,900              True\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7]      [32, 352, 7, 7]      1,888,920            True\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7]      [32, 1408, 7, 7]     --                   True\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7]      [32, 1408, 7, 7]     495,616              True\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7]     [32, 1408, 7, 7]     2,816                True\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7]     [32, 1408, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7]     [32, 1408, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1408]           [32, 1000]           --                   True\n",
       "│    └─Dropout (0)                                           [32, 1408]           [32, 1408]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1408]           [32, 1000]           1,409,000            True\n",
       "============================================================================================================================================\n",
       "Total params: 9,109,994\n",
       "Trainable params: 9,109,994\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 21.09\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.79\n",
       "Params size (MB): 36.44\n",
       "Estimated Total Size (MB): 5073.49\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(effnetB2,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b749d4b78058d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:04:35.325642Z",
     "start_time": "2025-12-15T09:04:35.280256Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "OUT_FEATURES = len(class_names)\n",
    "\n",
    "# Create an EffNetB0\n",
    "def create_effnetB0(device=device):\n",
    "    # Get the weights and setup a model\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # \"DEFAULT\" = best available\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # Freeze all base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "      # Change the classifier head\n",
    "\n",
    "    set_seeds()\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.2, inplace=True),\n",
    "        nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"effnetB0\"\n",
    "    print(f\"[INFO] Created new model: {model.name}\")\n",
    "    return model\n",
    "\n",
    "# Create an EffNetB2\n",
    "def create_effnetB2(device=device):\n",
    "    # Get the weights and setup a model\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # \"DEFAULT\" = best available\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    # Freeze all base model layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "      # Change the classifier head\n",
    "\n",
    "    set_seeds()\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3, inplace=True),\n",
    "        nn.Linear(in_features=1408, out_features=len(class_names), bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    # Give the model a name\n",
    "    model.name = \"effnetB2\"\n",
    "    print(f\"[INFO] Created new model: {model.name}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2101ed60ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "effnetB2.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5e149a5bb8e4d297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:34:00.736926Z",
     "start_time": "2025-12-15T08:33:58.992463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Created new model: effnetB0\n",
      "[INFO] Created new model: effnetB2\n"
     ]
    }
   ],
   "source": [
    "created_model_test_effnetB0 = create_effnetB0()\n",
    "created_model_test_effnetB2 = create_effnetB2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ada06994bf5d1ef1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:34:03.099560Z",
     "start_time": "2025-12-15T08:34:00.766677Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(created_model_test_effnetB0,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1fcb53a39bccd5d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:34:06.674803Z",
     "start_time": "2025-12-15T08:34:03.158245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1408, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112]   [32, 16, 112, 112]   (612)                False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 48, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 48, 28, 28]     (16,518)             False\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     (43,308)             False\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     (43,308)             False\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28]     [32, 88, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28]     [32, 88, 14, 14]     (50,300)             False\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14]     [32, 120, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14]     [32, 120, 14, 14]    (149,158)            False\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14]    [32, 208, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14]    [32, 208, 7, 7]      (301,406)            False\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7]      [32, 352, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7]      [32, 352, 7, 7]      (846,900)            False\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7]      [32, 352, 7, 7]      (1,888,920)          False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7]      [32, 1408, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7]      [32, 1408, 7, 7]     (495,616)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7]     [32, 1408, 7, 7]     (2,816)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7]     [32, 1408, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7]     [32, 1408, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1408]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1408]           [32, 1408]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1408]           [32, 3]              4,227                True\n",
       "============================================================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (Units.GIGABYTES): 21.04\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.53\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 5067.62\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(created_model_test_effnetB2,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8d61fb3ba1be6",
   "metadata": {},
   "source": [
    "### 7.6 Create experiments and set up training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8c228aaa4c52848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T09:04:50.001777Z",
     "start_time": "2025-12-15T09:04:49.954222Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create epoch list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# Create models list ( need to create a new model for each experiment)\n",
    "models = [\"effnetB0\", \"effnetB2\"]\n",
    "\n",
    "# Create a DataLoaders dictionary\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n",
    "                     \"data_20_percent\": train_dataloader_20_percent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c3d62a2f53d539",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-15T09:04:50.423427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment 0 | Number of Epochs 5 | Model: effnetB0 | DataLoader: data_10_percent\n",
      "[INFO] Created new model: effnetB0\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_10_percent/effnetB0/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59ac0952aed4308aa83d8babf6fb05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0427 | train_acc: 0.5000 | test_loss: 0.9384 | test_acc: 0.4886\n",
      "Epoch: 2 | train_loss: 0.9193 | train_acc: 0.5664 | test_loss: 0.8126 | test_acc: 0.7017\n",
      "Epoch: 3 | train_loss: 0.7880 | train_acc: 0.7344 | test_loss: 0.6786 | test_acc: 0.8864\n",
      "Epoch: 4 | train_loss: 0.7088 | train_acc: 0.7578 | test_loss: 0.5752 | test_acc: 0.9375\n",
      "Epoch: 5 | train_loss: 0.5777 | train_acc: 0.8984 | test_loss: 0.5424 | test_acc: 0.9280\n",
      "[INFO] Saving model to: models/07_effnetB0_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 1 | Number of Epochs 5 | Model: effnetB2 | DataLoader: data_10_percent\n",
      "[INFO] Created new model: effnetB2\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_10_percent/effnetB2/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b36d99d122e4d91a81995508433dbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0778 | train_acc: 0.3828 | test_loss: 0.9241 | test_acc: 0.7027\n",
      "Epoch: 2 | train_loss: 0.8731 | train_acc: 0.6523 | test_loss: 0.8491 | test_acc: 0.7746\n",
      "Epoch: 3 | train_loss: 0.8241 | train_acc: 0.6836 | test_loss: 0.7525 | test_acc: 0.7955\n",
      "Epoch: 4 | train_loss: 0.6975 | train_acc: 0.8633 | test_loss: 0.6319 | test_acc: 0.8968\n",
      "Epoch: 5 | train_loss: 0.7063 | train_acc: 0.7461 | test_loss: 0.5752 | test_acc: 0.8968\n",
      "[INFO] Saving model to: models/07_effnetB2_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 2 | Number of Epochs 10 | Model: effnetB0 | DataLoader: data_10_percent\n",
      "[INFO] Created new model: effnetB0\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_10_percent/effnetB0/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2df9d630c34678a1a9266175ffc975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0427 | train_acc: 0.5000 | test_loss: 0.9384 | test_acc: 0.4886\n",
      "Epoch: 2 | train_loss: 0.9193 | train_acc: 0.5664 | test_loss: 0.8126 | test_acc: 0.7017\n",
      "Epoch: 3 | train_loss: 0.7880 | train_acc: 0.7344 | test_loss: 0.6786 | test_acc: 0.8864\n",
      "Epoch: 4 | train_loss: 0.7088 | train_acc: 0.7578 | test_loss: 0.5752 | test_acc: 0.9375\n",
      "Epoch: 5 | train_loss: 0.5777 | train_acc: 0.8984 | test_loss: 0.5424 | test_acc: 0.9280\n",
      "Epoch: 6 | train_loss: 0.5564 | train_acc: 0.8711 | test_loss: 0.5753 | test_acc: 0.8665\n",
      "Epoch: 7 | train_loss: 0.6094 | train_acc: 0.7383 | test_loss: 0.5773 | test_acc: 0.8153\n",
      "Epoch: 8 | train_loss: 0.4828 | train_acc: 0.9219 | test_loss: 0.4920 | test_acc: 0.9271\n",
      "Epoch: 9 | train_loss: 0.4853 | train_acc: 0.9531 | test_loss: 0.4581 | test_acc: 0.9271\n",
      "Epoch: 10 | train_loss: 0.5806 | train_acc: 0.7852 | test_loss: 0.4222 | test_acc: 0.9271\n",
      "[INFO] Saving model to: models/07_effnetB0_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 3 | Number of Epochs 10 | Model: effnetB2 | DataLoader: data_10_percent\n",
      "[INFO] Created new model: effnetB2\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_10_percent/effnetB2/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc353d7f10b04573a37a735bc66524be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0778 | train_acc: 0.3828 | test_loss: 0.9241 | test_acc: 0.7027\n",
      "Epoch: 2 | train_loss: 0.8731 | train_acc: 0.6523 | test_loss: 0.8491 | test_acc: 0.7746\n",
      "Epoch: 3 | train_loss: 0.8241 | train_acc: 0.6836 | test_loss: 0.7525 | test_acc: 0.7955\n",
      "Epoch: 4 | train_loss: 0.6975 | train_acc: 0.8633 | test_loss: 0.6319 | test_acc: 0.8968\n",
      "Epoch: 5 | train_loss: 0.7063 | train_acc: 0.7461 | test_loss: 0.5752 | test_acc: 0.8968\n",
      "Epoch: 6 | train_loss: 0.5833 | train_acc: 0.9180 | test_loss: 0.5921 | test_acc: 0.8873\n",
      "Epoch: 7 | train_loss: 0.5872 | train_acc: 0.8086 | test_loss: 0.5439 | test_acc: 0.9280\n",
      "Epoch: 8 | train_loss: 0.5197 | train_acc: 0.8281 | test_loss: 0.5317 | test_acc: 0.8977\n",
      "Epoch: 9 | train_loss: 0.4757 | train_acc: 0.9062 | test_loss: 0.4805 | test_acc: 0.9280\n",
      "Epoch: 10 | train_loss: 0.5111 | train_acc: 0.8320 | test_loss: 0.4900 | test_acc: 0.8977\n",
      "[INFO] Saving model to: models/07_effnetB2_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 4 | Number of Epochs 5 | Model: effnetB0 | DataLoader: data_20_percent\n",
      "[INFO] Created new model: effnetB0\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_20_percent/effnetB0/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c7b30dacf475aae59d499fa8ee0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9425 | train_acc: 0.5896 | test_loss: 0.6304 | test_acc: 0.9167\n",
      "Epoch: 2 | train_loss: 0.6905 | train_acc: 0.8021 | test_loss: 0.5376 | test_acc: 0.9176\n",
      "Epoch: 3 | train_loss: 0.5547 | train_acc: 0.8396 | test_loss: 0.4534 | test_acc: 0.9176\n",
      "Epoch: 4 | train_loss: 0.4524 | train_acc: 0.8979 | test_loss: 0.3699 | test_acc: 0.9688\n",
      "Epoch: 5 | train_loss: 0.4354 | train_acc: 0.8979 | test_loss: 0.3449 | test_acc: 0.9489\n",
      "[INFO] Saving model to: models/07_effnetB0_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 5 | Number of Epochs 5 | Model: effnetB2 | DataLoader: data_20_percent\n",
      "[INFO] Created new model: effnetB2\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_20_percent/effnetB2/5_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aab207d72164692ab3c009d59b837d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9705 | train_acc: 0.5958 | test_loss: 0.7325 | test_acc: 0.8968\n",
      "Epoch: 2 | train_loss: 0.6960 | train_acc: 0.8083 | test_loss: 0.6149 | test_acc: 0.8873\n",
      "Epoch: 3 | train_loss: 0.5320 | train_acc: 0.8896 | test_loss: 0.4976 | test_acc: 0.8977\n",
      "Epoch: 4 | train_loss: 0.4987 | train_acc: 0.8667 | test_loss: 0.4719 | test_acc: 0.9081\n",
      "Epoch: 5 | train_loss: 0.4588 | train_acc: 0.8292 | test_loss: 0.3887 | test_acc: 0.9280\n",
      "[INFO] Saving model to: models/07_effnetB2_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment 6 | Number of Epochs 10 | Model: effnetB0 | DataLoader: data_20_percent\n",
      "[INFO] Created new model: effnetB0\n",
      "[INFO] Created SummaryWriter saving to runs/2025-12-15/data_20_percent/effnetB0/10_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe482ea423143a4b2e30bfd7621a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "jetTransient": {
      "display_id": null
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9425 | train_acc: 0.5896 | test_loss: 0.6304 | test_acc: 0.9167\n",
      "Epoch: 2 | train_loss: 0.6905 | train_acc: 0.8021 | test_loss: 0.5376 | test_acc: 0.9176\n",
      "Epoch: 3 | train_loss: 0.5547 | train_acc: 0.8396 | test_loss: 0.4534 | test_acc: 0.9176\n",
      "Epoch: 4 | train_loss: 0.4524 | train_acc: 0.8979 | test_loss: 0.3699 | test_acc: 0.9688\n",
      "Epoch: 5 | train_loss: 0.4354 | train_acc: 0.8979 | test_loss: 0.3449 | test_acc: 0.9489\n",
      "Epoch: 6 | train_loss: 0.3987 | train_acc: 0.9000 | test_loss: 0.3116 | test_acc: 0.9176\n",
      "Epoch: 7 | train_loss: 0.3499 | train_acc: 0.9104 | test_loss: 0.2795 | test_acc: 0.9280\n",
      "Epoch: 8 | train_loss: 0.3392 | train_acc: 0.8917 | test_loss: 0.2911 | test_acc: 0.9280\n",
      "Epoch: 9 | train_loss: 0.3411 | train_acc: 0.9000 | test_loss: 0.2938 | test_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from going_modular.utils import save_model\n",
    "\n",
    "# Set seeds\n",
    "set_seeds()\n",
    "\n",
    "# Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# Loop through each DataLoader\n",
    "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "    # Loop through the epochs\n",
    "    for epochs in num_epochs:\n",
    "        # Loop through each model name and create a new model instance\n",
    "        for model_name in models:\n",
    "            if experiment_number in range(6):\n",
    "                experiment_number+=1\n",
    "                continue\n",
    "            # Print out the info\n",
    "            print(f\"[INFO] Experiment {experiment_number} | Number of Epochs {epochs} | Model: {model_name} | DataLoader: {dataloader_name}\")\n",
    "\n",
    "            if model_name == \"effnetB0\":\n",
    "                model = create_effnetB0(device=device)\n",
    "            else:\n",
    "                model = create_effnetB2(device=device)\n",
    "\n",
    "            # Create a new loss and optimizer\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(lr=0.001, params=model.parameters())\n",
    "\n",
    "            # Train target model with target dataloader and track experiments\n",
    "            writer = create_writer(\n",
    "                experiment_name= dataloader_name,\n",
    "                model_name=model_name,\n",
    "                extra=f\"{epochs}_epochs\",\n",
    "            )\n",
    "\n",
    "            train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, writer)\n",
    "\n",
    "            # Save the model to file so we can import it late if needed\n",
    "            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            del model\n",
    "            print(\"-\"*50 + \"\\n\")\n",
    "            experiment_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e10d319-e5ac-435b-8da7-0fd062736ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293ca1a-9c0b-421d-870f-6190f06b9ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c673e77223d1386",
   "metadata": {},
   "source": [
    "## 8. View experiments in TensorBoard\n",
    "\n",
    "We've experimented...\n",
    "\n",
    "Now let's visualize :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba91a5-b78d-444a-baa9-c9a5df6f1326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bd2053904d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view our experiments within TensorBoard from within the notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567311ff409a668a",
   "metadata": {},
   "source": [
    "The best performing model was:\n",
    "* Model: EffNetB2\n",
    "* Dataset: Pizza, steak, sushi 20%\n",
    "* Epochs: 10\n",
    "\n",
    "And the overall trend of all the results was that more data, bigger model and longer training time generally led to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd49b8a311a61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the results to TensorBoard.dev (uncomment to try it out)\n",
    "!tensorboard dev upload --logdir runs --name \"07. PyTorch Experiment Tracking: FoodVision Mini model result\" --description \"Comparing results of different model size, training data amount and training time.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1937dab8839c94",
   "metadata": {},
   "source": [
    "You can view the experiments publicly at TensorBoard.dev: {insert the link}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce94d5d5f6797280",
   "metadata": {},
   "source": [
    "# 9. Load in the best modal and create predictions with it\n",
    "This is our best model filepath: `{THe file pth}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450c813c3930a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup best model file path\n",
    "best_model_path = \"\"\n",
    "\n",
    "# Instantiate a new instance of EFFnetB? (to load the saved state_dict())\n",
    "best_model = create_effnetB0()\n",
    "\n",
    "# Load the saved best model state_dict()\n",
    "best_model.load_state_dict(torch.load(best_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541501981e25e919",
   "metadata": {},
   "source": [
    "Our goal: create a FoodVision Mini model that performs well enough and is able to run on a mobile device/web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8f56cb5b33ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model file size\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the model sioze in bytes then convert it to megabytes\n",
    "effnetb2_model_size = Path(best_model_path).stat().st_size // (1_048_576)\n",
    "print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec81fb29c9b52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function to make prediction on images and plot them\n",
    "from going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "# Get a random list of 3 image path names from the test dataset\n",
    "import random\n",
    "num_images_to_plot = 3\n",
    "test_image_paths_list = list(Path(data_20_percent_path/ \"test\").glob(\"*/*.jpg\"))\n",
    "test_image_paths_sample = random.sample(test_image_paths_list,\n",
    "                                       k=num_images_to_plot)\n",
    "\n",
    "for image_path in test_image_paths_sample:\n",
    "    pred_and_plot_image(\n",
    "        model=best_model,\n",
    "        image_path=image_path,\n",
    "        image_size=(224,224),\n",
    "        class_names=class_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103678de1c44d08",
   "metadata": {},
   "source": [
    "## 9.1 Predict on a custom image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5026b4df94412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_and_plot_image(\n",
    "    model=best_model,\n",
    "    image_path=Path(\"data/04-pizza-dad.jpeg\"),\n",
    "    class_names=class_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0c4486d58b81b",
   "metadata": {},
   "source": [
    "# \\# Progress updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "805b4d2a3757ab1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:52:36.378877Z",
     "start_time": "2025-12-15T08:52:36.255287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[92mUpdated progress report.\u001b[0m \n",
      "\u001b[1mVideo:\u001b[0m 216. Coding Out the Steps to Run a Series of Modelling Experiments \n",
      "\u001b[1mDuration:\u001b[0m 14m \n",
      "\u001b[1mStatus:\u001b[0m Done \n",
      "\u001b[1mDate:\u001b[0m 15 Dec 2025 10:52 AM \n",
      "\u001b[1mSection progress:\u001b[0m \n",
      "\u001b[1mSection :\u001b[0m 9.PyTorch Experiment Tracking\n",
      "\u001b[1mStatus  :\u001b[0m 5 videos remaining, 37m to finish the section\n",
      "\u001b[1mProgress: \u001b[0m 77%|\u001b[34m██████████████████████▍      \u001b[0m 17/22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from Progress.course_progress_func import update_progress, progress_pie_chart, monthly_progress, progress_report_print\n",
    "update_progress(video_index=221 , done=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58976d7d407a9cca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:34:45.541634Z",
     "start_time": "2025-12-15T08:34:45.436921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[94m      ___         ___           ___           ___           ___           ___           ___           ___     \n",
      "     /  /\\       /  /\\         /  /\\         /  /\\         /  /\\         /  /\\         /  /\\         /  /\\    \n",
      "    /  /::\\     /  /::\\       /  /::\\       /  /:/_       /  /::\\       /  /:/_       /  /:/_       /  /:/_   \n",
      "   /  /:/\\:\\   /  /:/\\:\\     /  /:/\\:\\     /  /:/ /\\     /  /:/\\:\\     /  /:/ /\\     /  /:/ /\\     /  /:/ /\\  \n",
      "  /  /:/~/:/  /  /:/~/:/    /  /:/  \\:\\   /  /:/_/::\\   /  /:/~/:/    /  /:/ /:/_   /  /:/ /::\\   /  /:/ /::\\ \n",
      " /__/:/ /:/  /__/:/ /:/___ /__/:/ \\__\\:\\ /__/:/__\\/\\:\\ /__/:/ /:/___ /__/:/ /:/ /\\ /__/:/ /:/\\:\\ /__/:/ /:/\\:\\\n",
      " \\  \\:\\/:/   \\  \\:\\/:::::/ \\  \\:\\ /  /:/ \\  \\:\\ /~~/:/ \\  \\:\\/:::::/ \\  \\:\\/:/ /:/ \\  \\:\\/:/~/:/ \\  \\:\\/:/~/:/\n",
      "  \\  \\::/     \\  \\::/~~~~   \\  \\:\\  /:/   \\  \\:\\  /:/   \\  \\::/~~~~   \\  \\::/ /:/   \\  \\::/ /:/   \\  \\::/ /:/ \n",
      "   \\  \\:\\      \\  \\:\\        \\  \\:\\/:/     \\  \\:\\/:/     \\  \\:\\        \\  \\:\\/:/     \\__\\/ /:/     \\__\\/ /:/  \n",
      "    \\  \\:\\      \\  \\:\\        \\  \\::/       \\  \\::/       \\  \\:\\        \\  \\::/        /__/:/        /__/:/   \n",
      "     \\__\\/       \\__\\/         \\__\\/         \\__\\/         \\__\\/         \\__\\/         \\__\\/         \\__\\/    \n",
      "      ___           ___           ___         ___           ___                 \n",
      "     /  /\\         /  /\\         /  /\\       /  /\\         /  /\\          ___   \n",
      "    /  /::\\       /  /:/_       /  /::\\     /  /::\\       /  /::\\        /  /\\  \n",
      "   /  /:/\\:\\     /  /:/ /\\     /  /:/\\:\\   /  /:/\\:\\     /  /:/\\:\\      /  /:/  \n",
      "  /  /:/~/:/    /  /:/ /:/_   /  /:/~/:/  /  /:/  \\:\\   /  /:/~/:/     /  /:/   \n",
      " /__/:/ /:/___ /__/:/ /:/ /\\ /__/:/ /:/  /__/:/ \\__\\:\\ /__/:/ /:/___  /  /::\\   \n",
      " \\  \\:\\/:::::/ \\  \\:\\/:/ /:/ \\  \\:\\/:/   \\  \\:\\ /  /:/ \\  \\:\\/:::::/ /__/:/\\:\\  \n",
      "  \\  \\::/~~~~   \\  \\::/ /:/   \\  \\::/     \\  \\:\\  /:/   \\  \\::/~~~~  \\__\\/  \\:\\ \n",
      "   \\  \\:\\        \\  \\:\\/:/     \\  \\:\\      \\  \\:\\/:/     \\  \\:\\           \\  \\:\\\n",
      "    \\  \\:\\        \\  \\::/       \\  \\:\\      \\  \\::/       \\  \\:\\           \\__\\/\n",
      "     \\__\\/         \\__\\/         \\__\\/       \\__\\/         \\__\\/                \n",
      "\u001b[0m\n",
      "\u001b[1m\u001b[93mCourse:\u001b[0m  74%|\u001b[34m████████████████████       \u001b[0m 266/358\u001b[0m\n",
      "\u001b[1m\u001b[93mTotal time done:\u001b[0m 40h 32m out of 52h 15m watched (11h 43m remaining).\n",
      "\u001b[1m\u001b[93mTotal videos done:\u001b[0m 266 out of 358 finished (92 videos remaining).\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 1.Introduction\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (7 videos | Total Duration: 22m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m███████████████████████████████\u001b[0m 7/7\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 2.PyTorch Fundamentals\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (32 videos | Total Duration: 4h 9m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 32/32\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 3.PyTorch Workflow\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (28 videos | Total Duration: 4h 17m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 28/28\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 4.PyTorch Neural Network Classification\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (32 videos | Total Duration: 5h 26m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 32/32\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 5.PyTorch Computer Vision\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (34 videos | Total Duration: 5h 42m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 34/34\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 6.PyTorch Custom Datasets\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (37 videos | Total Duration: 5h 51m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 37/37\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 7.PyTorch Going Modular\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (10 videos | Total Duration: 1h 33m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 10/10\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 8.PyTorch Transfer Learning\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (19 videos | Total Duration: 2h 44m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 19/19\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 9.PyTorch Experiment Tracking\n",
      "\u001b[1mStatus  :\u001b[0m 6 videos remaining, 51m to finish the section\n",
      "\u001b[1mProgress: \u001b[0m 73%|\u001b[34m█████████████████████        \u001b[0m 16/22\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 10.PyTorch Paper Replicating (Skipped for now)\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (50 videos | Total Duration: 8h 7m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m█████████████████████████████\u001b[0m 50/50\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 11.PyTorch Model Deployment\n",
      "\u001b[1mStatus  :\u001b[0m 57 videos remaining, 7h 45m to finish the section\n",
      "\u001b[1mProgress: \u001b[0m  0%|\u001b[34m                              \u001b[0m 0/57\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 12.Introduction to PyTorch 2.0 and torch.compile\n",
      "\u001b[1mStatus  :\u001b[0m 25 videos remaining, 3h 3m to finish the section\n",
      "\u001b[1mProgress: \u001b[0m  0%|\u001b[34m                              \u001b[0m 0/25\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 13.Bonus Section\n",
      "\u001b[1mStatus  :\u001b[0m \u001b[92mDone (1 video | Total Duration: 1m)\u001b[0m\n",
      "\u001b[1mProgress: \u001b[0m100%|\u001b[34m███████████████████████████████\u001b[0m 1/1\u001b[0m\n",
      "\n",
      "\u001b[1mSection :\u001b[0m 14.Where To Go From Here?\n",
      "\u001b[1mStatus  :\u001b[0m 4 videos remaining, 4m to finish the section\n",
      "\u001b[1mProgress: \u001b[0m  0%|\u001b[34m                               \u001b[0m 0/4\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "progress_report_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
